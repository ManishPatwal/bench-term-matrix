Term Matrix Comparison
======================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.height = 6, fig.width = 11.7,
                      fig.retina = TRUE)
options(width = 120, max.print = 100)
library(ggplot2)
theme_set(theme_gray(base_size = 18))
library(quanteda); quanteda_options(threads = 1)
```

Overview
--------

There are multiple R packages that can transform text data into a
matrix of term frequency counts. This document benchmarks five
packages:

 + [corpus](https://github.com/patperry/r-corpus)
 + [quanteda](https://github.com/kbenoit/quanteda)
 + [text2vec](http://text2vec.org/)
 + [tidytext](https://github.com/juliasilge/tidytext)
 + [tm](http://tm.r-forge.r-project.org/)

There are four benchmarks, two for unigrams only, and two for unigrams and
bigrams. In each benchmark, we perform the following sequence of operations:

 + case fold the text
 + tokenize into words
 + remove punctuation
 + remove numbers
 + remove stop words
 + stem
 + compute bigrams (bigram benchmarks only)
 + compute term frequencies
 + remove terms that appear fewer than five times in the corpus
 + compute a term frequency matrix (text by term)
 
There are some subtle and not-so-subtle differences in how the five packages
implement these operations, so this is not really an apples-to-apples
comparison, and the outputs are different. Keep that in mind.



Prelude
-------

We will load the following packages.

```{r}
library("Matrix")
library("dplyr", warn.conflicts = FALSE)
library("ggplot2")
library("magrittr")
library("methods")
library("stringr")
```

The remaining packages need to be installed, but we will not load their
namespaces:

```{r}
# Note: we use a development versions of corpus, installed
# by following the instructions at
# https://github.com/patperry/r-corpus#building-from-source
#
# Not run:
# install.packages(c("microbenchmark", "quanteda", "text2vec", "tidytext", "tm"))
```

For the first test corpus, we use the 62 chapters from *Pride and Prejudice*,
provided by the
[janeaustenr](https://github.com/juliasilge/janeaustenr) library:

```{r}
lines <- (data_frame(text = janeaustenr::prideprejudice)
          %>% mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                            ignore_case = TRUE)))))
text_novel <- c(tapply(lines$text, lines$chapter, paste, collapse = "\n"))
```

For the second test corpus, we use the 5000 movie reviews provided by the
*text2vec* package:

```{r}
text_reviews <- text2vec::movie_review$review
names(text_reviews) <- text2vec::movie_review$id
```

We will use the Snowball English stop word list:
```{r}
stop_words <- corpus::stopwords("english")
```


Implementations
---------------

### Basic

As a baseline, we will include a basic implementation, written from scratch by
Dmitriy Selivanov (*text2vec* author) that can handle unigrams but not
bigrams:

```{r}

# helper function for normalizing text, also used by text2vec below
preprocess <- function(x)
{
    # Note: this works fine for ASCII but not for general Unicode.
    # For Unicode, do the following instead:
    #
    # (stringi::stri_trans_nfkc_casefold(x)
    #  %>% stringi::stri_replace_all_regex("[^\\p{Letter}\\s]", ""))

    str_to_lower(x) %>% str_replace_all("[^[:alpha:]\\s]", "")
}

# helper function for tokenizing and stemming, also used by text2vec below
stem_tokenizer <- function(x)
{
    str_split(x, boundary("word")) %>% lapply(SnowballC::wordStem, "english")
}

matrix_basic <- function(text, min_count = 5)
{
    # normalize and tokenize the text
    toks <- text %>% preprocess %>% stem_tokenizer
    toks_flat <- unlist(toks, recursive = FALSE, use.names = FALSE)

    # compute the text lengths
    ntok <- vapply(toks, length, 0L)

    # compute the types, remove stop words
    types <- unique(toks_flat) %>% setdiff(stop_words)

    # construct the term matrix
    i <- rep.int(seq_along(text), ntok)
    j <- match(toks_flat, types)
    drop <- is.na(j)
    x <- sparseMatrix(i = i[!drop], j = j[!drop], x = 1,
                      dims = c(length(text), length(types)),
                      dimnames = list(names(text), types),
                      check = FALSE)

    # drop terms below the minimum count
    x <- x[, colSums(x) >= min_count, drop = FALSE]
    x
}
```


### corpus

```{r}
matrix_corpus <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngrams <- 1:2
    } else {
        ngrams <- 1
    }
    f <- corpus::text_filter(stemmer = "english", drop_punct = TRUE,
                             drop_number = TRUE, drop = stop_words)
    stats <- corpus::term_stats(text, f, ngrams = ngrams,
                                min_count = min_count)
    x <- corpus::term_matrix(text, f, select = stats$term)
    x
}
```


### quanteda

```{r}
matrix_quanteda <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngrams <- 1:2
    } else {
        ngrams <- 1
    }
    x <- quanteda::dfm(text, stem = TRUE, remove_punct = TRUE,
                       remove_numbers = TRUE, remove = stop_words,
                       ngrams = ngrams, verbose = FALSE)
    x <- quanteda::dfm_trim(x, min_count = min_count, verbose = FALSE)
    x
}
```


### text2vec

```{r}
# Written by Dmitriy Selivanov
matrix_text2vec <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngram <- c(1, 2)
    } else {
        ngram <- c(1, 1)
    }

    # since we don't care about RAM usage we will tokenize everything only
    # once and do it with a single call to preprocess and tokenizer
    tokens <- preprocess(text) %>% stem_tokenizer
  
    it_train <- text2vec::itoken(tokens, n_chunks = 1, progressbar = FALSE)
    vocab <- text2vec::create_vocabulary(it_train, ngram = ngram,
                                         stopwords = stop_words)
    pruned_vocab <- text2vec::prune_vocabulary(vocab,
                                               term_count_min = min_count)
    vectorizer <- text2vec::vocab_vectorizer(pruned_vocab)
    x <- text2vec::create_dtm(it_train, vectorizer)
    x
}
```


### tidytext

```{r}
# Note: this filters punctuation but keeps numbers
matrix_tidytext <- function(text, bigrams = FALSE, min_count = 5)
{
    data <- tibble::tibble(text_id = seq_along(text), text = text)
    stops <- tibble::tibble(word = stop_words)

    # unigrams
    freqs <- (data
        %>% tidytext::unnest_tokens(word, text)
        %>% anti_join(stops, by = "word")
        %>% mutate(term = SnowballC::wordStem(word, "english"))
        %>% count(text_id, term)
        %>% ungroup())

    # bigrams
    if  (bigrams) {
        freqs2 <- (data
            %>% tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2)
            %>% tidyr::separate(bigram, c("type1", "type2"), sep = " ")
            %>% filter(!type1 %in% stop_words,
                       !type2 %in% stop_words)
            %>% mutate(type1 = SnowballC::wordStem(type1, "english"),
                       type2 = SnowballC::wordStem(type2, "english"))
            %>% mutate(term = paste(type1, type2))
            %>% count(text_id, term)
            %>% ungroup())

        freqs <- rbind(freqs, freqs2)
    }

    # form matrix in slam format
    x <- freqs %>% tidytext::cast_dtm(text_id, term, n)

    # remove rare terms
    x <- x[, slam::col_sums(x) >= min_count, drop = FALSE]

    # cast to dgCMatrix format
    x <- sparseMatrix(i = x$i, j = x$j, x = x$v, dims = dim(x),
                      dimnames = dimnames(x), check = FALSE)
    x
}
```


### tm


```{r}
# from http://tm.r-forge.r-project.org/faq.html#Bigrams
BigramTokenizer <- function(x)
{
    unlist(lapply(NLP::ngrams(NLP::words(x), 2), paste, collapse = " "),
           use.names = FALSE)
}

matrix_tm <- function(text, bigrams = FALSE, min_count = 5)
{
    corpus <- (tm::VCorpus(tm::VectorSource(text))
               %>% tm::tm_map(tm::content_transformer(tolower))
               %>% tm::tm_map(tm::removeWords, stop_words)
               %>% tm::tm_map(tm::removePunctuation)
               %>% tm::tm_map(tm::removeNumbers)
               %>% tm::tm_map(tm::stemDocument, language = "english"))

    control <- list(wordLengths = c(1, Inf),
                    bounds = list(global = c(min_count, Inf)))

    x <- tm::DocumentTermMatrix(corpus, control = control)

    if (bigrams) {
        control$tokenize <- BigramTokenizer
        x2 <- tm::DocumentTermMatrix(corpus, control = control)

        x <- cbind(x, x2)
    }

    x <- sparseMatrix(i = x$i, j = x$j, x = x$v, dims = dim(x),
                      dimnames = dimnames(x), check = FALSE)
    x
}
```


Caveats
-------

These implementations all give different results. See, for example, the
results on the following sample text:

```{r}
sample <- "Above ground. Another sentence. Others..."

# compute term matrices using five implementations
xs <- list(
    corpus   = matrix_corpus(sample, bigrams = TRUE, min_count = 1),
    quanteda = matrix_quanteda(sample, bigrams = TRUE, min_count = 1),
    text2vec = matrix_text2vec(sample, bigrams = TRUE, min_count = 1),
    tidytext = matrix_tidytext(sample, bigrams = TRUE, min_count = 1),
    tm       = matrix_tm(sample, bigrams = TRUE, min_count = 1))

# normalize the names (some use '_' to join bigrams, others use ' ')
for (i in seq_along(xs)) {
    colnames(xs[[i]]) <- str_replace_all(colnames(xs[[i]]), " ", "_")
}

# get the unique terms
terms <- unique(c(sapply(xs, colnames), recursive = TRUE))

# put unigrams before bigrams, then order lexicographically
terms <- terms[order(str_count(terms, "_"), terms)]

# combine everything into a single matrix
x <- matrix(0, length(xs), length(terms), dimnames = list(names(xs), terms))
for (i in seq_along(xs)) {
    xi <- xs[[i]]
    x[i, colnames(xi)] <- as.numeric(xi[1, ])
}

print(as(x, "dgCMatrix"))
print(sample)
```

Some major differences between the implementations:

 1. With the *quanteda*, *tidytext*, and *tm* implementations, we remove stop
    words first, and then stem. With *text2vec*, we stem and then remove
    stop words. *Corpus* removes stop words after stemming and by default
    does not stem any words on the drop list. The word "other" is a stop
    word, but "others" is not.  However, "others" stems to "other".
    *Corpus* and *text2vec* remove "others"; *quanteda*, *tidytext*,
    and *tm* replace "others" with a non-dropped "other" token.
    Another example: "above" is a stop word that stems to "abov".
    *Text2vec* replaces "above" with "abov" and keeps the token; the
    other packages drop "above".

 2. By design, *corpus* does not form bigrams across dropped tokens, in
    particular across dropped punctuation. The other packagages form
    bigrams from "ground. Another" and "sentence. Others"; corpus does
    not.

There are also differences in how the packages handle numbers and punctuation.
Beyond that, there are differences in the default output formats, but we have
converted everything to the *Matrix* `"dgCMatrix"` format to make the outputs
comparable. (By default, *corpus*, *quanteda*, and *text2vec* return *Matrix*
objects, but *tidytext* and *tm* return *slam* objects.)


Results
-------

### Setup

First we benchmark the implementations:

```{r}
make_bench <- function(name, text, bigrams)
{
    if (!bigrams) {
        results <- microbenchmark::microbenchmark (
            basic = matrix_basic(text),
            corpus = matrix_corpus(text, bigrams = FALSE),
            quanteda = matrix_quanteda(text, bigrams = FALSE),
            text2vec = matrix_text2vec(text, bigrams = FALSE),
            tidytext = matrix_tidytext(text, bigrams = FALSE),
            tm = matrix_tm(text, bigrams = FALSE),
            times = 5)
    } else {
        results <- microbenchmark::microbenchmark (
            corpus = matrix_corpus(text, bigrams = TRUE),
            quanteda = matrix_quanteda(text, bigrams = TRUE),
            text2vec = matrix_text2vec(text, bigrams = TRUE),
            tidytext = matrix_tidytext(text, bigrams = TRUE),
            tm = matrix_tm(text, bigrams = TRUE),
            times = 5)
    }

    list(name = name, results = results)
}

plot_bench <- function(bench, title)
{
    (ggplot(summary(bench$results),
            aes(x = expr, fill = expr, y = median, ymin = lq, ymax = uq))
     + geom_bar(color = "white", stat = "identity")
     + geom_errorbar()
     + scale_fill_discrete(name = "Implementation")
     + xlab("")
     + ylab("Computation time (less is better)"))
}
```

Next, we present the results for the four benchmarks.


### Unigrams (novel)

```{r}
bench1 <- make_bench("Unigram, Novel", text_novel, bigrams = FALSE)
plot_bench(bench1)
print(bench1$results)
```


### Unigrams (reviews)

```{r}
bench2 <- make_bench("Unigram, Reviews", text_reviews, bigrams = FALSE)
plot_bench(bench2)
print(bench2$results)
```


### Bigrams (novel)

```{r}
bench3 <- make_bench("Bigram, Novel", text_novel, bigrams = TRUE)
plot_bench(bench3)
print(bench3$results)
```

### Bigrams (reviews)

```{r}
bench4 <- make_bench("Bigram, Reviews", text_reviews, bigrams = TRUE)
plot_bench(bench4)
print(bench4$results)
```

Summary
-------

*Corpus* is faster than the other packages, by at least a factor of 2 and as
much as a factor of 10. What's going on here? The other packages tokenize the
text into a list of character vectors, then the process the token lists to
form the term matrices. *Corpus* instead bypasses the intermediate step, going
directly from the text to the term matrix without constructing an intermediate
"tokens" object. This is only possible because all of the *corpus*
normalization and tokenization is written directly in C.

The downside of the *corpus* approach is flexibility: if you're using
*corpus*, you can't swap out the normalization or tokenizer for something
custom. With varying degrees of ease, the other packages let you swap out
these steps for your own custom functions.

Of course, there's more to text mining than just term matrices, so if you
need more, than *corpus* alone probably won't be sufficient for you. The other
packages have different strengths: *quanteda* and *text2vec* provide a host of
models and metrics; *tidytext* fits in well with "tidy data" pipelines built
on *dplyr* and related tools; *tm* has lots of extension packages for data
input and modeling. Choose the package that best needs your needs.


Session information
-------------------

```{r}
sessionInfo()
```
