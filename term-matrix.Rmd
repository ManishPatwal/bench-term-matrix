Term Matrix Speed Contest
=========================

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.height = 6, fig.width = 11.7,
                      fig.retina = TRUE)
options(width = 120, max.print = 100)
library(ggplot2)
theme_set(theme_gray(base_size = 18))
library(quanteda); quanteda_options(threads = 1)
```

Overview
--------

There are multiple R packages that can transform text data into a
matrix of term frequency counts. This document benchmarks five
packages:

 + [corpus](https://github.com/patperry/r-corpus)
 + [quanteda](https://github.com/kbenoit/quanteda)
 + [text2vec](http://text2vec.org/)
 + [tidytext](https://github.com/juliasilge/tidytext)
 + [tm](http://tm.r-forge.r-project.org/)

There are four benchmarks, two for unigrams only, and two for unigrams and
bigrams. In each benchmark, we perform the following sequence of operations:

 + case fold the text
 + tokenize into words
 + remove puncuation
 + remove numbers
 + remove stop words
 + stem
 + compute bigrams (bigram benchmarks only)
 + compute term frequencies
 + remove terms that appear fewer than five times in the corpus
 + compute a term frequency matrix (text by term)
 
There are some subtle and not-so-subtle differences in how the five packages
implement these operations, so this is not really an apples-to-apples
comparison, and the outputs are different. Keep that in mind.


Prelude
-------

We will load the following packages.

```{r}
library("Matrix")
library("dplyr", warn.conflicts = FALSE)
library("ggplot2")
library("magrittr")
library("methods")
library("stringr")
```

The remaining packages need to be installed, but we will not load their
namespaces:

```{r}
# Not run:
# install.packages(c("microbenchmark", "quanteda", "tidytext", "tm"))
# install.packages("https://github.com/patperry/r-corpus/raw/master/dist/corpus_0.6.1.tar.gz", repos = NULL) 
# devtools::install_github("dselivanov/text2vec@5a778fb517082c4a4a69f84dd5e9d045a18bc0bf")
```

For the first test corpus, we use the 62 chapters from *Pride and Prejudice*,
provided by the
[janeaustenr](https://github.com/juliasilge/janeaustenr) library:

```{r}
lines <- (data_frame(text = janeaustenr::prideprejudice)
          %>% mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                            ignore_case = TRUE)))))
text_novel <- c(tapply(lines$text, lines$chapter, paste, collapse = "\n"))
```

For the second test corpus, we use the 5000 movie reviews provided by the
*text2vec* package:

```{r}
text_reviews <- text2vec::movie_review$review
names(text_reviews) <- text2vec::movie_review$id
```

We will use the Snowball English stop word list:
```{r}
stop_words <- corpus::stopwords("english")
```


Implementations
---------------

### Basic

As a baseline, we will include a basic implementation, written from scratch by
Dmitriy Selivanov (*text2vec* author) that can handle unigrams but not
bigrams:

```{r}

# helper function for normalizing text, also used by text2vec below
preprocess <- function(x)
{
    # Note: this works fine for ASCII but not for general Unicode.
    # For Unicode, do the following instead:
    #
    # (stringi::stri_trans_nfkc_casefold(x)
    #  %>% stringi::stri_replace_all_regex("[^\\p{Letter}\\s]", ""))

    str_to_lower(x) %>% str_replace_all("[^[:alpha:]\\s]", "")
}

# helper function for tokenizing and stemming, also used by text2vec below
stem_tokenizer <- function(x)
{
    str_split(x, boundary("word")) %>% lapply(SnowballC::wordStem, "english")
}

matrix_basic <- function(text, min_count = 5)
{
    # normalize and tokenize the text
    toks <- text %>% preprocess %>% stem_tokenizer
    toks_flat <- unlist(toks, recursive = FALSE, use.names = FALSE)

    # compute the text lengths
    ntok <- vapply(toks, length, 0L)

    # compute the types, remove stop words
    types <- unique(toks_flat) %>% setdiff(stop_words)

    # construct the term matrix
    i <- rep.int(seq_along(text), ntok)
    j <- match(toks_flat, types)
    drop <- is.na(j)
    x <- sparseMatrix(i = i[!drop], j = j[!drop], x = 1,
                      dims = c(length(text), length(types)),
                      dimnames = list(names(text), types),
                      check = FALSE)

    # drop terms below the minimum count
    x <- x[, colSums(x) >= min_count, drop = FALSE]
    x
}
```


### corpus

```{r}
matrix_corpus <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngrams <- 1:2
    } else {
        ngrams <- 1
    }
    f <- corpus::token_filter(stemmer = "english", drop_punct = TRUE,
                              drop_number = TRUE, drop = stop_words)
    stats <- corpus::term_counts(text, f, ngrams = ngrams, min = min_count)
    x <- corpus::term_matrix(text, f, select = stats$term)
    x
}
```


### quanteda

```{r}
matrix_quanteda <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngrams <- 1:2
    } else {
        ngrams <- 1
    }
    x <- quanteda:::dfm(text, stem = TRUE, remove_punct = TRUE,
                       remove_numbers = TRUE, remove = stop_words,
                       ngrams = ngrams, verbose = FALSE)
    x <- quanteda::dfm_trim(x, min_count = min_count, verbose = FALSE)
    x
}
```


### text2vec

```{r}
# Written by Dmitriy Selivanov
matrix_text2vec <- function(text, bigrams = FALSE, min_count = 5)
{
    if (bigrams) {
        ngram <- c(1, 2)
    } else {
        ngram <- c(1, 1)
    }

    # since we don't care about RAM usage we will tokenize everything only
    # once and do it with a single call to preprocess and tokenizer
    tokens <- preprocess(text) %>% stem_tokenizer
  
    it_train <- text2vec::itoken(tokens, n_chunks = 1, progressbar = FALSE)
    vocab <- text2vec::create_vocabulary(it_train, ngram = ngram,
                                         stopwords = stop_words)
    pruned_vocab <- text2vec::prune_vocabulary(vocab,
                                               term_count_min = min_count)
    vectorizer <- text2vec::vocab_vectorizer(pruned_vocab)
    x <- text2vec::create_dtm(it_train, vectorizer)
    x
}
```


### tidytext

```{r}
# Note: this filters punctuation but keeps numbers
matrix_tidytext <- function(text, bigrams = FALSE, min_count = 5)
{
    data <- tibble::tibble(text_id = seq_along(text), text = text)
    stops <- tibble::tibble(word = stop_words)

    # unigrams
    freqs <- (data
        %>% tidytext::unnest_tokens(word, text)
        %>% anti_join(stops, by = "word")
        %>% mutate(term = SnowballC::wordStem(word, "english"))
        %>% count(text_id, term)
        %>% ungroup())

    # bigrams
    if  (bigrams) {
        freqs2 <- (data
            %>% tidytext::unnest_tokens(bigram, text, token = "ngrams", n = 2)
            %>% tidyr::separate(bigram, c("type1", "type2"), sep = " ")
            %>% filter(!type1 %in% stop_words,
                       !type2 %in% stop_words)
            %>% mutate(type1 = SnowballC::wordStem(type1, "english"),
                       type2 = SnowballC::wordStem(type2, "english"))
            %>% mutate(term = paste(type1, type2))
            %>% count(text_id, term)
            %>% ungroup())

        freqs <- rbind(freqs, freqs2)
    }

    # form matrix in slam format
    x <- freqs %>% tidytext::cast_dtm(text_id, term, n)

    # remove rare terms
    x <- x[, slam::col_sums(x) >= min_count, drop = FALSE]

    # cast to dgCMatrix format
    x <- sparseMatrix(i = x$i, j = x$j, x = x$v, dims = dim(x),
                      dimnames = dimnames(x), check = FALSE)
    x
}
```


### tm


```{r}
# from http://tm.r-forge.r-project.org/faq.html#Bigrams
BigramTokenizer <- function(x)
{
    unlist(lapply(NLP::ngrams(NLP::words(x), 2), paste, collapse = " "),
           use.names = FALSE)
}

matrix_tm <- function(text, bigrams = FALSE, min_count = 5)
{
    corpus <- (tm::VCorpus(tm::VectorSource(text))
               %>% tm::tm_map(tm::content_transformer(tolower))
               %>% tm::tm_map(tm::removeWords, stop_words)
               %>% tm::tm_map(tm::removePunctuation)
               %>% tm::tm_map(tm::removeNumbers)
               %>% tm::tm_map(tm::stemDocument, language = "english"))

    control <- list(wordLengths = c(1, Inf),
                    bounds = list(global = c(min_count, Inf)))

    x <- tm::DocumentTermMatrix(corpus, control = control)

    if (bigrams) {
        control$tokenize <- BigramTokenizer
        x2 <- tm::DocumentTermMatrix(corpus, control = control)

        x <- cbind(x, x2)
    }

    x <- sparseMatrix(i = x$i, j = x$j, x = x$v, dims = dim(x),
                      dimnames = dimnames(x), check = FALSE)
    x
}
```


Caveats
-------

These implementations all give different results. See, for example, the
results on the following sample text:

```{r}
sample <- "Above ground. Another sentence. Others..."

# compute term matrices using five implementations
xs <- list(
    corpus   = matrix_corpus(sample, bigrams = TRUE, min_count = 1),
    quanteda = matrix_quanteda(sample, bigrams = TRUE, min_count = 1),
    text2vec = matrix_text2vec(sample, bigrams = TRUE, min_count = 1),
    tidytext = matrix_tidytext(sample, bigrams = TRUE, min_count = 1),
    tm       = matrix_tm(sample, bigrams = TRUE, min_count = 1))

# normalize the names (some use '_' to join bigrams, others use ' ')
for (i in seq_along(xs)) {
    colnames(xs[[i]]) <- str_replace_all(colnames(xs[[i]]), " ", "_")
}

# get the unique terms
terms <- unique(c(sapply(xs, colnames), recursive = TRUE))

# put unigrams before bigrams, then order lexicographically
terms <- terms[order(str_count(terms, "_"), terms)]

# combine everything into a single matrix
x <- matrix(0, length(xs), length(terms), dimnames = list(names(xs), terms))
for (i in seq_along(xs)) {
    xi <- xs[[i]]
    x[i, colnames(xi)] <- as.numeric(xi[1, ])
}

print(as(x, "dgCMatrix"))
print(sample)
```

Some major differences between the implementations:

 1. With the *quanteda*, *tidytext*, and *tm* implementations, we remove stop
    words first, and then stem. With *text2vec*, we stem and then remove
    stop words. *Corpus* removes stop words after stemming and by default
    does not stem any words on the drop list. The word "other" is a stop
    word, but "others" is not.  However, "others" stems to "other".
    *Corpus* and *text2vec* remove "others"; *quanteda*, *tidytext*,
    and *tm* replace "others" with a non-dropped "other" token.
    Another example: "above" is a stop word that stems to "abov".
    *Text2vec* replaces "above" with "abov" and keeps the token; the
    other packages drop "above".

 2. By design, *corpus* does not form bigrams across dropped tokens, in
    particular across dropped punctuation. The other packagages form
    bigrams from "ground. Another" and "sentence. Others"; corpus does
    not.

There are also differences in how the packages handle numbers and punctuation.
Beyond that, there are differences in the default output formats, but we have
converted everything to the *Matrix* `"dgCMatrix"` format to make the outputs
comparable. (By default, *corpus*, *quanteda*, and *text2vec* return *Matrix*
objects, but *tidytext* and *tm* return *slam* objects.)


Results
-------

# Benchmarking

First we benchmark the implementations:

```{r}
make_bench <- function(name, text, bigrams)
{
    if (!bigrams) {
        results <- microbenchmark::microbenchmark (
            basic = matrix_basic(text),
            corpus = matrix_corpus(text, bigrams = FALSE),
            quanteda = matrix_quanteda(text, bigrams = FALSE),
            text2vec = matrix_text2vec(text, bigrams = FALSE),
            tidytext = matrix_tidytext(text, bigrams = FALSE),
            tm = matrix_tm(text, bigrams = FALSE),
        times = 5)
    } else {
        results <- microbenchmark::microbenchmark (
            corpus = matrix_corpus(text, bigrams = FALSE),
            # quanteda = matrix_quanteda(text, bigrams = FALSE),
            text2vec = matrix_text2vec(text, bigrams = FALSE),
            tidytext = matrix_tidytext(text, bigrams = FALSE),
            tm = matrix_tm(text, bigrams = FALSE),
        times = 5)
    }
    list(name = name, results = results)
}

plot_bench <- function(bench, title)
{
    (ggplot(summary(bench$results),
            aes(x = expr, fill = expr, y = median, ymin = lq, ymax = uq))
     + geom_bar(color = "white", stat = "identity")
     + geom_errorbar()
     + scale_fill_discrete(name = "Implementation")
     + xlab("")
     + ylab("Computation time (less is better)")
     + ggtitle(bench$name))
}
```

### Unigrams (novel)

Here are the results for the unigram benchmark.

```{r}
bench1 <- make_bench("Unigram, Novel", text_novel, bigrams = FALSE)
plot_bench(bench1)
print(bench1$results)
```

### Unigrams (reviews)

```{r}
bench2 <- make_bench("Unigram, Reviews", text_reviews, bigrams = FALSE)
plot_bench(bench2)
print(bench2$results)
```


### Bigrams (novel)

```{r}
bench3 <- make_bench("Bigram, Novel", text_novel, bigrams = TRUE)
plot_bench(bench3)
print(bench3$results)
```

### Bigrams (reviews)

```{r}
bench4 <- make_bench("Bigram, Reviews", text_reviews, bigrams = TRUE)
plot_bench(bench4)
print(bench4$results)
```

Summary
-------

For the unigram benchmark, *corpus* is XXX times faster than *quanteda* and
*tidytext*, and XXX times faster than *text2vec* and *tm*.

For the bigram benchmark, *corpus* is XXX times faster than *text2vec* and
*tidytext*, XXX times faster than *tm*, and XXX times faster than
*quanteda*. (I'm not sure why the *quanteda* results are so bad, I
might be doing something wrong.)


Session information
-------------------

```{r}
sessionInfo()
```
